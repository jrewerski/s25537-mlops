{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faza 2 i 3: Od Notebooka do Zautomatyzowanego Potoku MLOps\n",
    "\n",
    "Ten notatnik stanowi praktyczny przewodnik po procesie przekształcania eksperymentalnego kodu z notatnika Jupyter w modularny, reużywalny i zautomatyzowany potok uczenia maszynowego przy użyciu **Kubeflow Pipelines (KFP)** i **Vertex AI Pipelines**.\n",
    "\n",
    "Celem jest demonstracja, jak logika z prototypu (Faza 1) zostaje podzielona na niezależne **komponenty**, a następnie połączona w spójny graf wykonania (**pipeline**), który zarządza całym cyklem życia modelu – od danych surowych po wdrożenie.\n",
    "\n",
    "### Kluczowe korzyści tego podejścia:\n",
    "\n",
    "* **Powtarzalność:** Każde uruchomienie potoku wykonuje te same kroki w identyczny sposób.\n",
    "* **Reużywalność:** Komponenty mogą być wykorzystywane w innych projektach.\n",
    "* **Skalowalność:** Poszczególne kroki mogą być uruchamiane na zasobach chmurowych o odpowiedniej mocy obliczeniowej.\n",
    "* **Łatwość w utrzymaniu:** Zmiany w jednym komponencie nie wpływają na inne, o ile interfejsy (wejścia/wyjścia) pozostają te same.\n",
    "* **Automatyzacja:** Potok stanowi podstawę dla systemów CI/CD, umożliwiając automatyczne trenowanie i wdrażanie modeli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Konfiguracja środowiska i import bibliotek\n",
    "\n",
    "Na początku importujemy niezbędne biblioteki, w tym KFP, oraz definiujemy zmienne konfiguracyjne dla naszego projektu w Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import Input, Output, Dataset, Model, Metrics\n",
    "\n",
    "# --- Zmienne konfiguracyjne ---\n",
    "# Uzupełnij poniższe zmienne swoimi wartościami z Google Cloud\n",
    "PROJECT_ID = \"twoj-google-cloud-project-id\"  # Wstaw swój Project ID\n",
    "BUCKET_NAME = \"gs://twoj-gcs-bucket-name\"     # Wstaw nazwę swojego bucketu w GCS\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline-root\" # Ścieżka do przechowywania artefaktów potoku\n",
    "REGION = \"europe-central2\"                   # Region, w którym działa Vertex AI, np. europe-central2 dla Warszawy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tworzenie Komponentów Potoku\n",
    "\n",
    "**Komponent KFP** to samodzielna część potoku, która realizuje jedno, konkretne zadanie. Jest to funkcja w języku Python, opatrzona dekoratorem `@dsl.component`. Dekorator ten zawiera metadane, takie jak obraz bazowy kontenera Docker oraz pakiety do zainstalowania. Dzięki temu każdy komponent jest uruchamiany w izolowanym, powtarzalnym środowisku.\n",
    "\n",
    "Poniżej zdefiniujemy komponenty dla każdego kroku naszego procesu ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 1: Pobieranie i ładowanie danych\n",
    "\n",
    "Ten komponent jest odpowiedzialny za pobranie surowego zbioru danych z publicznego zasobu Google Cloud Storage i przekazanie go jako artefakt typu `Dataset` do kolejnego kroku w potoku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"fsspec\", \"gcsfs\", \"scikit-learn\"]\n",
    ")\n",
    "def load_data(\n",
    "    dataset: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Pobiera dane o pingwinach i zapisuje je jako artefakt.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    csv_url = \"gs://cloud-samples-data/vertex-ai/pipeline-introduction/penguins.csv\"\n",
    "    df = pd.read_csv(csv_url)\n",
    "    \n",
    "    df.to_csv(dataset.path, index=False)\n",
    "    print(f\"Data loaded and saved to {dataset.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 2: Przygotowanie i podział danych\n",
    "\n",
    "Komponent ten przyjmuje surowe dane, wykonuje na nich operacje czyszczenia i inżynierii cech (zgodnie z logiką z notatnika), a następnie dzieli je na zbiory treningowe i testowe. Każdy z tych podzbiorów jest zapisywany jako osobny artefakt wyjściowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\"]\n",
    ")\n",
    "def preprocess_data(\n",
    "    dataset_in: Input[Dataset],\n",
    "    x_train: Output[Dataset],\n",
    "    x_test: Output[Dataset],\n",
    "    y_train: Output[Dataset],\n",
    "    y_test: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Czyści dane, dokonuje inżynierii cech i dzieli zbiór na treningowy i testowy.\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    df = pd.read_csv(dataset_in.path)\n",
    "\n",
    "    # Czyszczenie danych (zgodnie z notatnikiem)\n",
    "    df.loc[336, 'sex'] = 'FEMALE' # Poprawienie błędnej wartości\n",
    "    df_clean = df.copy()\n",
    "    numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "    for col in numerical_cols:\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    df_clean['sex'] = df_clean['sex'].fillna(df_clean['sex'].mode()[0])\n",
    "    \n",
    "    # Inżynieria cech\n",
    "    df_clean['sex'] = df_clean['sex'].map({'MALE': 0, 'FEMALE': 1})\n",
    "    df_processed = pd.get_dummies(df_clean, columns=['island'], drop_first=True)\n",
    "\n",
    "    # Zdefiniowanie cech (X) i etykiety (y)\n",
    "    X = df_processed.drop('species', axis=1)\n",
    "    y = df_processed['species']\n",
    "\n",
    "    # Podział na zbiór treningowy i testowy\n",
    "    X_train_df, X_test_df, y_train_ser, y_test_ser = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Zapisanie podzielonych danych jako artefakty\n",
    "    X_train_df.to_csv(x_train.path, index=False)\n",
    "    X_test_df.to_csv(x_test.path, index=False)\n",
    "    y_train_ser.to_csv(y_train.path, index=False, header=False)\n",
    "    y_test_ser.to_csv(y_test.path, index=False, header=False)\n",
    "    print(\"Data preprocessed and split successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 3: Trenowanie modelu\n",
    "\n",
    "Ten komponent odpowiada za proces trenowania. Przyjmuje dane treningowe i tworzy potok `scikit-learn` zawierający skaler i klasyfikator SVC. Wytrenowany model jest następnie zapisywany jako artefakt typu `Model`, co pozwala na jego wersjonowanie i śledzenie w Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"]\n",
    ")\n",
    "def train_model(\n",
    "    x_train_in: Input[Dataset],\n",
    "    y_train_in: Input[Dataset],\n",
    "    model_out: Output[Model]\n",
    "):\n",
    "    \"\"\"Trenuje model SVC przy użyciu potoku scikit-learn.\"\"\"\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.pipeline import Pipeline\n",
    "\n",
    "    X_train = pd.read_csv(x_train_in.path)\n",
    "    y_train = pd.read_csv(y_train_in.path, header=None).squeeze()\n",
    "\n",
    "    # Definicja potoku scikit-learn\n",
    "    svc_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svc', SVC(kernel='linear', probability=True, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Trenowanie modelu\n",
    "    svc_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Zapisanie modelu jako artefakt\n",
    "    model_out.metadata[\"framework\"] = \"scikit-learn\"\n",
    "    model_out.metadata[\"containerSpec\"] = {\n",
    "        \"imageUri\": \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\"\n",
    "    }\n",
    "    joblib.dump(svc_pipeline, model_out.path)\n",
    "    print(f\"Model trained and saved to {model_out.path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 4: Ocena modelu\n",
    "\n",
    "Po wytrenowaniu model musi zostać oceniony. Ten komponent wczytuje model oraz dane testowe, dokonuje predykcji i oblicza metrykę dokładności (*accuracy*). Wynik jest zapisywany jako artefakt typu `Metrics`, co umożliwia jego wizualizację w interfejsie Vertex AI Pipelines i wykorzystanie w krokach warunkowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"]\n",
    ")\n",
    "def evaluate_model(\n",
    "    x_test_in: Input[Dataset],\n",
    "    y_test_in: Input[Dataset],\n",
    "    model_in: Input[Model],\n",
    "    metrics_out: Output[Metrics]\n",
    "):\n",
    "    \"\"\"Ocenia model na zbiorze testowym i zapisuje metryki.\"\"\"\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    X_test = pd.read_csv(x_test_in.path)\n",
    "    y_test = pd.read_csv(y_test_in.path, header=None).squeeze()\n",
    "    \n",
    "    model = joblib.load(model_in.path)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Zapisanie metryk jako artefakt\n",
    "    metrics_out.log_metric(\"accuracy\", accuracy)\n",
    "    print(f\"Model accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 5: Warunkowe wdrożenie modelu\n",
    "\n",
    "Ostatni komponent odpowiada za proces ciągłego dostarczania (CD). Jego zadaniem jest wdrożenie modelu na **Vertex AI Endpoint**, ale tylko wtedy, gdy spełniony jest określony warunek – w tym przypadku, gdy dokładność modelu na zbiorze testowym przekracza zdefiniowany próg. Komponent ten wykorzystuje SDK `google-cloud-aiplatform` do interakcji z usługami Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"]\n",
    ")\n",
    "def deploy_model_to_vertex(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_in: Input[Model],\n",
    "    metrics_in: Input[Metrics],\n",
    "    min_accuracy: float,\n",
    "    endpoint_out: Output[dsl.Artifact]\n",
    "):\n",
    "    \"\"\"Wdraża model na Vertex AI Endpoint, jeśli dokładność jest wystarczająca.\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    \n",
    "    accuracy = metrics_in.metadata[\"accuracy\"]\n",
    "    print(f\"Current model accuracy: {accuracy}\")\n",
    "\n",
    "    if accuracy >= min_accuracy:\n",
    "        print(f\"Accuracy {accuracy} >= {min_accuracy}. Deploying model.\")\n",
    "        \n",
    "        aiplatform.init(project=project, location=location)\n",
    "\n",
    "        # Wgranie modelu do Vertex AI Model Registry\n",
    "        uploaded_model = aiplatform.Model.upload(\n",
    "            display_name=\"penguin-classifier-pipeline\",\n",
    "            artifact_uri=model_in.uri.replace(\"/model\", \"\"), # Wymagany jest URI do katalogu\n",
    "            serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\"\n",
    "        )\n",
    "        \n",
    "        # Stworzenie i wdrożenie punktu końcowego (endpoint)\n",
    "        endpoint = uploaded_model.deploy(\n",
    "            machine_type=\"n1-standard-2\",\n",
    "            min_replica_count=1,\n",
    "            max_replica_count=1,\n",
    "            traffic_split={\"0\": 100},\n",
    "            sync=True\n",
    "        )\n",
    "        \n",
    "        endpoint_out.uri = endpoint.resource_name\n",
    "        print(f\"Model deployed to endpoint: {endpoint.resource_name}\")\n",
    "    else:\n",
    "        print(f\"Accuracy {accuracy} < {min_accuracy}. Skipping deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definicja i Kompilacja Potoku\n",
    "\n",
    "Mając zdefiniowane wszystkie komponenty, możemy teraz połączyć je w jeden, spójny potok. Funkcja oznaczona dekoratorem `@dsl.pipeline` definiuje graf wykonania (DAG), określając, które komponenty zależą od wyników (artefaktów) innych.\n",
    "\n",
    "W naszym przypadku:\n",
    "1.  `load_data` uruchamia się jako pierwszy.\n",
    "2.  `preprocess_data` czeka na dane z `load_data`.\n",
    "3.  `train_model` czeka na dane treningowe z `preprocess_data`.\n",
    "4.  `evaluate_model` czeka na model z `train_task` i dane testowe z `preprocess_data`.\n",
    "5.  `deploy_model_to_vertex` jest uruchamiany warunkowo, na podstawie metryki z `evaluate_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"penguin-classification-pipeline\",\n",
    "    description=\"Zautomatyzowany potok ML do klasyfikacji pingwinów.\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def penguin_classification_pipeline(\n",
    "    min_accuracy_threshold: float = 0.95,\n",
    "    project_id: str = PROJECT_ID,\n",
    "    region: str = REGION\n",
    "):\n",
    "    \"\"\"Orkiestruje przepływ pracy od danych do wdrożonego modelu.\"\"\"\n",
    "    \n",
    "    load_task = load_data()\n",
    "\n",
    "    preprocess_task = preprocess_data(\n",
    "        dataset_in=load_task.outputs[\"dataset\"]\n",
    "    )\n",
    "\n",
    "    train_task = train_model(\n",
    "        x_train_in=preprocess_task.outputs[\"x_train\"],\n",
    "        y_train_in=preprocess_task.outputs[\"y_train\"]\n",
    "    )\n",
    "\n",
    "    evaluate_task = evaluate_model(\n",
    "        x_test_in=preprocess_task.outputs[\"x_test\"],\n",
    "        y_test_in=preprocess_task.outputs[\"y_test\"],\n",
    "        model_in=train_task.outputs[\"model_out\"]\n",
    "    )\n",
    "\n",
    "    # Krok warunkowy\n",
    "    with dsl.Condition(\n",
    "        evaluate_task.outputs[\"metrics_out\"].metadata[\"accuracy\"] >= min_accuracy_threshold,\n",
    "        name=\"deploy-condition\",\n",
    "    ):\n",
    "        deploy_task = deploy_model_to_vertex(\n",
    "            project=project_id,\n",
    "            location=region,\n",
    "            model_in=train_task.outputs[\"model_out\"],\n",
    "            metrics_in=evaluate_task.outputs[\"metrics_out\"],\n",
    "            min_accuracy=min_accuracy_threshold\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Kompilacja i Uruchomienie\n",
    "\n",
    "Ostatnim krokiem jest skompilowanie definicji potoku do pliku w formacie JSON. Ten plik zawiera wszystkie informacje potrzebne usłudze Vertex AI Pipelines do wykonania naszego potoku. \n",
    "\n",
    "Po skompilowaniu, plik `penguin_pipeline.json` można wgrać do interfejsu Vertex AI, aby uruchomić zadanie, lub zrobić to programistycznie za pomocą SDK, jak pokazano w zakomentowanym kodzie poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Kompilacja potoku do pliku JSON\n",
    "    kfp.compiler.Compiler().compile(\n",
    "        pipeline_func=penguin_classification_pipeline,\n",
    "        package_path=\"penguin_pipeline.json\"\n",
    "    )\n",
    "    print(\"Pipeline compiled to penguin_pipeline.json\")\n",
    "\n",
    "    # Uruchomienie potoku w Vertex AI (wymaga uwierzytelnienia w gcloud)\n",
    "    # from google.cloud import aiplatform\n",
    "    # \n",
    "    # aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "    # \n",
    "    # job = aiplatform.PipelineJob(\n",
    "    #     display_name=\"penguin-pipeline-run\",\n",
    "    #     template_path=\"penguin_pipeline.json\",\n",
    "    #     pipeline_root=PIPELINE_ROOT,\n",
    "    #     enable_caching=True\n",
    "    # )\n",
    "    # \n",
    "    # job.run()\n",
    "    # print(\"Pipeline job started.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
