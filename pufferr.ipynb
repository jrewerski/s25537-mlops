{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faza 2 i 3: Budowa Potoku MLOps z Modularnych Komponentów\n",
    "\n",
    "Ten notatnik stanowi kontynuację procesu ewolucji projektu MLOps. W Fazie 1 dokonano eksploracji danych i stworzono prototyp w notatniku Jupyter. Teraz, w Fazie 2, logika ta została wyodrębniona do samodzielnych, reużywalnych skryptów w języku Python, które pełnią rolę komponentów. W Fazie 3 skupiamy się na zdefiniowaniu **potoku (pipeline)** w **Vertex AI** przy użyciu biblioteki **Kubeflow Pipelines (KFP)**, który orkiestruje te komponenty.\n",
    "\n",
    "Celem jest zademonstrowanie, jak luźno powiązane skrypty można zintegrować w spójny, zautomatyzowany i łatwy w utrzymaniu system. Potok będzie realizował następujące kroki:\n",
    "1.  **Pobranie danych** z Google Cloud Storage.\n",
    "2.  **Przetworzenie danych:** czyszczenie, imputacja braków i podział na zbiory treningowe/testowe.\n",
    "3.  **Trenowanie modelu** klasyfikatora SVC.\n",
    "4.  **Ewaluacja modelu** w oparciu o metrykę dokładności.\n",
    "5.  **Pobranie informacji o poprzedniej wersji modelu** (jeśli istnieje).\n",
    "6.  **Warunkowa rejestracja nowego modelu** w Vertex AI Model Registry, jeśli jego dokładność przekroczy zdefiniowany próg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Definicje Komponentów\n",
    "\n",
    "Poniżej znajdują się definicje poszczególnych komponentów potoku. Każdy komponent to funkcja w języku Python opakowana dekoratorem `@component` z biblioteki KFP. Dekorator ten definiuje środowisko wykonawcze (obraz bazowy i pakiety), w którym zostanie uruchomiony kod komponentu. Takie podejście zapewnia izolację i powtarzalność każdego kroku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 1: Pobieranie Danych (`get_data.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import Artifact, Dataset, Input, Output, Component\n",
    "\n",
    "@Component(\n",
    "    packages_to_install=[\"pandas==2.2.2\", \"gcsfs\", \"fsspec\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\"\n",
    ")\n",
    "def get_data(gcs_input_path: str, input_data: Output[Dataset]):\n",
    "    import pandas as pd \n",
    "    # Wczytanie danych z podanej ścieżki GCS\n",
    "    df = pd.read_csv(gcs_input_path)\n",
    "    # Zapisanie danych jako artefakt wyjściowy\n",
    "    df.to_csv(input_data.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 2: Przetwarzanie Wstępne Danych (`preprocess_data.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Component(\n",
    "    packages_to_install=[\"pandas==2.2.2\", \"scikit-learn==1.5.0\", \"pyarrow\"],\n",
    "    base_image=\"python:3.9\"\n",
    ")\n",
    "def preprocess_data(\n",
    "    input_data: Input[Dataset],\n",
    "    train_dataset: Output[Dataset],\n",
    "    test_dataset: Output[Dataset],\n",
    "    test_split_ratio: float,\n",
    "):\n",
    "    \"\"\"Czyści, imputuje, dzieli i zapisuje dane jako artefakty treningowe/testowe.\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    pd.options.mode.chained_assignment = None\n",
    "\n",
    "    df = pd.read_csv(input_data.path)\n",
    "    df.loc[336, 'sex'] = 'FEMALE'\n",
    "    numerical_cols = ['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "    for col in numerical_cols:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "    df['sex'] = df['sex'].fillna(df['sex'].mode()[0])\n",
    "    df['sex'] = df['sex'].map({'MALE': 0, 'FEMALE': 1})\n",
    "    df_processed = pd.get_dummies(df, columns=['island'], drop_first=True)\n",
    "    \n",
    "    X = df_processed.drop('species', axis=1)\n",
    "    y = df_processed['species']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split_ratio, random_state=42, stratify=y)\n",
    "    \n",
    "    train_df = pd.concat([X_train, y_train], axis=1)\n",
    "    test_df = pd.concat([X_test, y_test], axis=1)\n",
    "    \n",
    "    # Zapis do artefaktów wyjściowych\n",
    "    train_df.to_csv(train_dataset.path, index=False)\n",
    "    test_df.to_csv(test_dataset.path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 3: Trenowanie Modelu (`train_svc_model.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas==2.2.2\", \"scikit-learn==1.5.0\", \"gcsfs==2024.6.0\", \"fsspec\", \"pyarrow\"]\n",
    ")\n",
    "def train_svc_model(\n",
    "    train_dataset: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "):\n",
    "    \"\"\"Trenuje klasyfikator SVC i zapisuje model.\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    import pickle\n",
    "\n",
    "    train_df = pd.read_csv(train_dataset.path)\n",
    "    X_train = train_df.drop('species', axis=1)\n",
    "    y_train = train_df['species']\n",
    "    \n",
    "    svc_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svc', SVC(kernel='rbf', probability=True, random_state=42))\n",
    "    ])\n",
    "    svc_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Zapisanie modelu do pliku .pkl w ścieżce artefaktu\n",
    "    file_name = model.path + \".pkl\"\n",
    "    with open(file_name, 'wb') as file:  \n",
    "        pickle.dump(svc_pipeline, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 4: Ewaluacja Modelu (`evaluate_svc_model.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "@Component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"pandas==2.2.2\", \"scikit-learn==1.5.0\", \"gcsfs==2024.6.0\", \"fsspec\"],\n",
    ")\n",
    "def evaluate_svc_model(\n",
    "    test_dataset: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics],\n",
    ") -> NamedTuple(\"Outputs\", [(\"accuracy\", float)]):\n",
    "    \"\"\"Ocenia model, zapisuje metryki i zwraca dokładność.\"\"\"\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    import pickle \n",
    "\n",
    "    file_name = model.path + \".pkl\"\n",
    "    with open(file_name, 'rb') as file:  \n",
    "        model = pickle.load(file)\n",
    "\n",
    "    test_df = pd.read_csv(test_dataset.path)\n",
    "    X_test = test_df.drop('species', axis=1)\n",
    "    y_test = test_df['species']\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Zapisanie metryk do artefaktu Vertex AI\n",
    "    metrics.log_metric(\"accuracy\", (accuracy * 100.0))\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    for class_label, class_metrics in report.items():\n",
    "        if isinstance(class_metrics, dict):\n",
    "            for metric_name, metric_value in class_metrics.items():\n",
    "                metrics.log_metric(f\"{class_label}_{metric_name}\", metric_value)\n",
    "    \n",
    "    # Zwrócenie dokładności jako wartości wyjściowej komponentu\n",
    "    return ((accuracy * 100.0),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 5: Pobranie Modelu Nadrzędnego (`get_parent_model.py`)\n",
    "\n",
    "Ten komponent służy do wersjonowania modeli. Sprawdza on w Vertex AI Model Registry, czy istnieje już model o podanej nazwie. Jeśli tak, zwraca jego identyfikator (resource name), który zostanie użyty do oznaczenia nowego modelu jako kolejnej wersji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\"]\n",
    ")\n",
    "def get_parent_model(\n",
    "    project: str,\n",
    "    region: str,\n",
    "    model_display_name: str,\n",
    ") -> NamedTuple(\"Outputs\", [(\"parent_model_resource_name\", str)]):\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    \n",
    "    models = aiplatform.Model.list(\n",
    "        filter=f'display_name=\"{model_display_name}\"'\n",
    "    )\n",
    "    \n",
    "    parent_model_resource_name = \"\"\n",
    "    if models:\n",
    "        parent_model_resource_name = models[0].resource_name\n",
    "        print(f\"Znaleziono istniejący model: {parent_model_resource_name}\")\n",
    "    else:\n",
    "        print(f\"Nie znaleziono modelu o nazwie: {model_display_name}. Zostanie utworzony nowy wpis.\")\n",
    "        \n",
    "    return (parent_model_resource_name,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Komponent 6: Rejestracja Modelu (`register_model.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Component(\n",
    "    base_image=\"python:3.9\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.55.0\"],\n",
    ")\n",
    "def register_model(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    model_display_name: str,\n",
    "    parent_model: str = \"\",\n",
    "    model_labels: str = '{}'\n",
    "):\n",
    "    \"\"\"Rejestruje model w Vertex AI Model Registry.\"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    import json\n",
    "\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    try:\n",
    "        labels = json.loads(model_labels)\n",
    "    except json.JSONDecodeError:\n",
    "        labels = {\"model_type\": \"svc\", \"framework\" : \"scikit-learn\"} # Domyślne etykiety\n",
    "\n",
    "    serving_container_image = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-5:latest\"\n",
    "    model_path = '/'.join(model.uri.split('/')[:-1])\n",
    "    \n",
    "    # Przesłanie i rejestracja modelu\n",
    "    registered_model = aiplatform.Model.upload(\n",
    "        display_name=model_display_name,\n",
    "        artifact_uri=model_path,\n",
    "        serving_container_image_uri=serving_container_image,\n",
    "        sync=True,\n",
    "        parent_model=parent_model,\n",
    "        labels=labels\n",
    "    )\n",
    "    print(f\"Zarejestrowano model: {registered_model.resource_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definicja i Kompilacja Głównego Potoku\n",
    "\n",
    "Po zdefiniowaniu wszystkich komponentów, łączymy je w jeden, spójny potok. Funkcja `training_pipeline` orkiestruje cały proces, definiując zależności i przepływ danych. Na końcu kompilujemy potok do pliku `pipeline.json`, który jest gotowy do uruchomienia w Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl, compiler\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"training-pipeline-from-components\",\n",
    "    description=\"Potok trenujący i rejestrujący model SVC z importowanych komponentów.\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "def training_pipeline(\n",
    "    gcs_data_path: str = \"gs://data-s25537/penguins.csv\",\n",
    "    project_id: str = PROJECT_ID,\n",
    "    region: str = REGION,\n",
    "    model_name: str = \"penguin-svc-model\",\n",
    "    model_labels_str: str = '{}',\n",
    "    test_split_ratio: float = 0.3,\n",
    "    min_accuracy_threshold: float = 95.0,\n",
    "):\n",
    "    \"\"\"Definiuje przepływ pracy w potoku z warunkową rejestracją modelu.\"\"\"\n",
    "    \n",
    "    get_data_task = get_data(gcs_input_path=gcs_data_path)\n",
    "    \n",
    "    transform_data_task = preprocess_data(\n",
    "        input_data=get_data_task.outputs[\"input_data\"],\n",
    "        test_split_ratio=test_split_ratio\n",
    "    )\n",
    "    \n",
    "    train_task = train_svc_model(\n",
    "        train_dataset=transform_data_task.outputs[\"train_dataset\"]\n",
    "    )\n",
    "    \n",
    "    evaluate_task = evaluate_svc_model(\n",
    "        test_dataset=transform_data_task.outputs[\"test_dataset\"],\n",
    "        model=train_task.outputs[\"model\"],\n",
    "    )\n",
    "\n",
    "    with dsl.If(\n",
    "        evaluate_task.outputs[\"accuracy\"] >= min_accuracy_threshold,\n",
    "        name=\"accuracy-check\",\n",
    "    ):\n",
    "        get_parent_model_task = get_parent_model(\n",
    "            project=project_id,\n",
    "            region=region,\n",
    "            model_display_name=model_name,\n",
    "        )\n",
    "        \n",
    "        register_model(\n",
    "            project_id=project_id,\n",
    "            region=region,\n",
    "            model_display_name=model_name,\n",
    "            model=train_task.outputs[\"model\"],\n",
    "            parent_model=get_parent_model_task.outputs[\"parent_model_resource_name\"],\n",
    "            model_labels=model_labels_str\n",
    "        )\n",
    "\n",
    "# Kompilacja potoku\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=training_pipeline,\n",
    "    package_path=\"training_pipeline.json\",\n",
    ")\n",
    "\n",
    "print(\"Potok został pomyślnie skompilowany do pliku 'training_pipeline.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
